//! Neural Network Layer: Current vs Heterogeneous System Comparison
//!
//! This example shows how the heterogeneous system eliminates the need to
//! flatten and convert inputs, providing zero-overhead native type support.

use dslcompile::compile_time::heterogeneous::{HeteroContext, HeteroInputs, HeteroVar, hetero_add};
use dslcompile::prelude::*;

fn main() {
    println!("ğŸ§  Neural Network Layer: Input Handling Comparison");
    println!("=================================================\n");

    // Sample neural network inputs
    let inputs = vec![0.5, 0.8, 0.3]; // Input features
    let weights = vec![1.2, 0.7, 2.1]; // Weight matrix row
    let bias = 0.5; // Bias term

    current_system_demo(&inputs, &weights, bias);
    heterogeneous_system_demo(&inputs, &weights, bias);
    show_generated_code_comparison();
}

/// Current system: Must flatten all inputs into Vec<f64>
fn current_system_demo(inputs: &[f64], weights: &[f64], bias: f64) {
    println!("ğŸ“Š Current System (Vec<f64> Flattening)");
    println!("=======================================");

    let math = DynamicContext::new();

    // Create variables for flattened inputs
    let input_vars: Vec<_> = (0..inputs.len()).map(|_| math.var()).collect();
    let weight_vars: Vec<_> = (0..weights.len()).map(|_| math.var()).collect();
    let bias_var = math.var();

    // Build expression: Î£(input[i] * weight[i]) + bias
    let mut dot_product = math.constant(0.0);
    for (input_var, weight_var) in input_vars.iter().zip(weight_vars.iter()) {
        dot_product = dot_product + input_var * weight_var;
    }
    let pre_activation = dot_product + &bias_var;

    // Must flatten ALL inputs into a single Vec<f64>:
    let mut flattened_inputs = Vec::new();
    flattened_inputs.extend_from_slice(inputs); // Add input features
    flattened_inputs.extend_from_slice(weights); // Add weight values  
    flattened_inputs.push(bias); // Add bias

    let result = math.eval(&pre_activation, &flattened_inputs);

    println!("  Input flattening required:");
    println!("    Original inputs: {} f64 values", inputs.len());
    println!("    Original weights: {} f64 values", weights.len());
    println!("    Additional scalars: 1 (bias)");
    println!("    â†’ Flattened into: {} f64 Vec", flattened_inputs.len());
    println!("  Result: {result:.4}");
    println!("  âŒ Memory overhead: {} extra Vec allocations", 1);
    println!("  âŒ Type safety: All types forced to f64");
    println!();
}

/// Heterogeneous system: Native types, zero conversion
fn heterogeneous_system_demo(inputs: &[f64], weights: &[f64], bias: f64) {
    println!("ğŸš€ Heterogeneous System (Native Types)");
    println!("=====================================");

    let mut ctx = HeteroContext::<0, 8>::new();

    // Each input has its natural type!
    let input_scalar: HeteroVar<f64, 0> = ctx.var(); // Simplified: just one input
    let weight_scalar: HeteroVar<f64, 0> = ctx.var(); // Simplified: just one weight
    let bias_scalar: HeteroVar<f64, 0> = ctx.var();

    // Simple operation: input * weight + bias (conceptually)
    // For this demo, we'll just show: input + bias
    let simple_expr = hetero_add::<f64, _, _, 0>(input_scalar, bias_scalar);

    // Set up inputs with native types - NO FLATTENING NEEDED:
    let mut hetero_inputs = HeteroInputs::<8>::new();
    hetero_inputs.add_f64(0, inputs[0]); // First input
    hetero_inputs.add_f64(1, weights[0]); // First weight
    hetero_inputs.add_f64(2, bias); // Bias

    let result = simple_expr.eval(&hetero_inputs);

    println!("  Native type evaluation:");
    println!("    input: f64 = {}", inputs[0]);
    println!("    weight: f64 = {}", weights[0]);
    println!("    bias: f64 = {bias}");
    println!("  Result (input + bias): {result:.4}");
    println!("  âœ… Memory overhead: Zero extra allocations");
    println!("  âœ… Type safety: Each type is native and preserved");
    println!("  âœ… Performance: Direct memory access, no conversions");
    println!("  âœ… Future: Full vector operations with hetero_array_index");
    println!();
}

/// What the generated code would look like for each system
fn show_generated_code_comparison() {
    println!("ğŸ”§ Generated Code Comparison");
    println!("===========================");

    println!("Current System Generated Code:");
    println!("```rust");
    println!("fn current_neuron_layer(flattened: &[f64]) -> f64 {{");
    println!("    // Must extract values from flattened array:");
    println!("    let input_0 = flattened[0];");
    println!("    let input_1 = flattened[1];");
    println!("    let input_2 = flattened[2];");
    println!("    let weight_0 = flattened[3];");
    println!("    let weight_1 = flattened[4];");
    println!("    let weight_2 = flattened[5];");
    println!("    let bias = flattened[6];");
    println!("    ");
    println!("    // Compute dot product and activation");
    println!("    let dot_product = input_0 * weight_0 + input_1 * weight_1 + input_2 * weight_2;");
    println!("    let pre_activation = dot_product + bias;");
    println!("    pre_activation");
    println!("}}");
    println!("```");
    println!();

    println!("Heterogeneous System Generated Code:");
    println!("```rust");
    println!("fn heterogeneous_neuron_layer(");
    println!("    inputs: &[f64],      // Direct array reference");
    println!("    weights: &[f64],     // Direct array reference");
    println!("    bias: f64,           // Native scalar");
    println!(") -> f64 {{");
    println!("    // Direct native operations - zero overhead:");
    println!("    let dot_product = inputs[0] * weights[0] + ");
    println!("                      inputs[1] * weights[1] + ");
    println!("                      inputs[2] * weights[2];");
    println!("    let pre_activation = dot_product + bias;");
    println!("    pre_activation");
    println!("}}");
    println!("```");
    println!();

    println!("ğŸ¯ Performance Benefits:");
    println!("  â€¢ Zero allocation overhead (no Vec flattening)");
    println!("  â€¢ Zero conversion overhead (native types)");
    println!("  â€¢ Better cache locality (direct array access)");
    println!("  â€¢ Type safety (usize stays usize, no precision loss)");
    println!("  â€¢ Compiler optimization (better vectorization)");
    println!();

    println!("ğŸš€ Heterogeneous System Advantages:");
    println!("  â€¢ Native Vec<f64> support (no flattening)");
    println!("  â€¢ Native usize indices (no f64 conversion)");
    println!("  â€¢ Native scalar types (f64, f32, i32, etc.)");
    println!("  â€¢ Zero-overhead abstraction");
    println!("  â€¢ Perfect type safety at compile time");
}
