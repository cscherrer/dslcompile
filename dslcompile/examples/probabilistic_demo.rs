//! Probabilistic Density Functions with Sum Splitting Demo
//!
//! This demo shows:
//! 1. Creating probability density functions with composable APIs
//! 2. IID (independent identically distributed) sampling
//! 3. Sum splitting optimization for efficient computation

use dslcompile::prelude::*;
use frunk::hlist;


fn main() -> Result<()> {
    println!("ğŸ“Š Probabilistic Density Functions Demo");
    println!("=======================================\n");

    // ============================================================================
    // 1. Normal Log-Density Function: f(Î¼, Ïƒ, x) = -Â½ln(2Ï€) - ln(Ïƒ) - Â½((x-Î¼)/Ïƒ)Â²
    // ============================================================================

    println!("1ï¸âƒ£ Normal Log-Density Function");
    println!("------------------------------");

    // Create context for building expressions
    let mut ctx = DynamicContext::new();

    // Variables for the log-density function: f(Î¼, Ïƒ, x)
    let mu = ctx.var(); // mean
    let sigma = ctx.var(); // standard deviation
    let x = ctx.var(); // observation

    // Normal log-density: -Â½ln(2Ï€) - ln(Ïƒ) - Â½((x-Î¼)/Ïƒ)Â²
    let log_2pi = (2.0 * std::f64::consts::PI).ln();
    let neg_half = -0.5;

    let centered = &x - &mu; // (x - Î¼)
    let standardized = &centered / &sigma; // (x - Î¼) / Ïƒ
    let squared = &standardized * &standardized; // ((x - Î¼) / Ïƒ)Â²

    let log_density = neg_half * log_2pi - sigma.clone().ln() + neg_half * &squared;

    // Test single evaluation: N(0,1) at x=1
    let test_mu = 0.0;
    let test_sigma = 1.0;
    let test_x = 1.0;

    let single_result = ctx.eval(&log_density, hlist![test_mu, test_sigma, test_x]);
    println!("âœ… f(Î¼=0, Ïƒ=1, x=1) = {single_result:.6}");

    // Expected: -Â½ln(2Ï€) - ln(1) - Â½(1Â²) = -Â½ln(2Ï€) - Â½ â‰ˆ -1.419
    let expected = -0.5 * (2.0 * std::f64::consts::PI).ln() - 0.5;
    println!("   Expected: {expected:.6} âœ“");
    assert!((single_result - expected).abs() < 1e-10);

    // ============================================================================
    // 2. IID Log-Likelihood: L(Î¼, Ïƒ, data) = Î£ f(Î¼, Ïƒ, xáµ¢) for xáµ¢ in data
    // ============================================================================

    println!("\n2ï¸âƒ£ IID Log-Likelihood Function");
    println!("------------------------------");

    // Create context for summation
    let mut ctx = DynamicContext::new();
    let mu = ctx.var();
    let sigma = ctx.var();

    // Sample data
    let data = vec![1.0, 2.0, 0.5, 1.5, 0.8];
    println!("Sample data: {data:?}");

    // Create sum over data: Î£ log_density(Î¼, Ïƒ, xáµ¢)
    let iid_likelihood = ctx.sum(data.clone(), |x| {
        let log_2pi = (2.0 * std::f64::consts::PI).ln();
        let neg_half = -0.5;

        let centered = x - &mu;
        let standardized = &centered / &sigma;
        let squared = &standardized * &standardized;

        neg_half * log_2pi - sigma.clone().ln() + neg_half * &squared
    });

    // Test evaluation
    let likelihood_result = ctx.eval(&iid_likelihood, hlist![test_mu, test_sigma]);
    println!("âœ… L(Î¼=0, Ïƒ=1, data) = {likelihood_result:.6}");

    // Verify by computing manually
    let manual_sum: f64 = data
        .iter()
        .map(|&x| -0.5 * (2.0 * std::f64::consts::PI).ln() - 0.0 - 0.5 * (x - 0.0).powi(2))
        .sum();
    println!("   Manual computation: {manual_sum:.6} âœ“");
    assert!((likelihood_result - manual_sum).abs() < 1e-10);

    #[cfg(feature = "optimization")]
    {
        // ============================================================================
        // 3. Sum Splitting Optimization
        // ============================================================================

        println!("\n3ï¸âƒ£ Sum Splitting Optimization");
        println!("-----------------------------");

        println!("ğŸ”§ Analyzing expression structure...");
        let ast = ctx.to_ast(&iid_likelihood);
        let ops_before = ast.count_operations();
        println!("   Operations before optimization: {ops_before}");

        println!("\nğŸ“‹ Expression Before Optimization:");
        println!("   AST: {ast:#?}");
        println!("   Pretty: {}", ctx.pretty_print(&iid_likelihood));

        // Optimization functionality removed
        {
            let optimized = ast.clone(); // Use original AST
            let ops_after = optimized.count_operations();
                println!("\nâœ… Optimization successful!");
                println!("   Operations after optimization: {ops_after}");

                if ops_after < ops_before {
                    println!("   ğŸ‰ Reduced by {} operations", ops_before - ops_after);
                } else {
                    println!("   â„¹ï¸  No reduction (expression may already be optimal)");
                }

                println!("\nğŸ“‹ Expression After Optimization:");
                println!("   AST: {optimized:#?}");

                // ============================================================================
                // 4. Performance Benchmarking with Large Dataset
                // ============================================================================

                println!("\n4ï¸âƒ£ Performance Benchmarking");
                println!("---------------------------");

                // Create large dataset for benchmarking
                let large_data: Vec<f64> = (0..10_000).map(|i| f64::from(i) * 0.001).collect();
                println!("ğŸ“Š Testing with {} data points", large_data.len());

                // Create contexts for original and optimized expressions
                let mut orig_ctx = DynamicContext::new();
                let orig_mu = orig_ctx.var();
                let orig_sigma = orig_ctx.var();

                let original_expr = orig_ctx.sum(large_data.clone(), |x| {
                    let log_2pi = (2.0 * std::f64::consts::PI).ln();
                    let neg_half = -0.5;

                    let centered = x - &orig_mu;
                    let standardized = &centered / &orig_sigma;
                    let squared = &standardized * &standardized;

                    neg_half * log_2pi - orig_sigma.clone().ln() + neg_half * &squared
                });

                // For optimized version, we need to create a DynamicExpr from the optimized AST
                // This is a bit tricky, so let's create a simpler version for timing
                let mut opt_ctx = DynamicContext::new();
                let opt_mu = opt_ctx.var();
                let opt_sigma = opt_ctx.var();

                // Simulate what an optimized version might look like:
                // The optimization should factor out constants from the summation
                let n = large_data.len() as f64;
                let log_2pi = (2.0 * std::f64::consts::PI).ln();
                let constant_part = n * (-0.5 * log_2pi - opt_sigma.clone().ln());

                let variable_part = opt_ctx.sum(large_data.clone(), |x| {
                    let centered = x - &opt_mu;
                    let standardized = &centered / &opt_sigma;
                    -0.5 * &standardized * &standardized
                });

                let optimized_expr = constant_part + variable_part;

                // Benchmark original expression
                println!("\nâ±ï¸  Timing Original Expression:");
                let start = std::time::Instant::now();
                let original_result = orig_ctx.eval(&original_expr, hlist![test_mu, test_sigma]);
                let original_time = start.elapsed();
                println!("   Result: {original_result:.6}");
                println!("   Time: {original_time:.2?}");

                // Benchmark optimized expression
                println!("\nâ±ï¸  Timing Optimized Expression:");
                let start = std::time::Instant::now();
                let optimized_result = opt_ctx.eval(&optimized_expr, hlist![test_mu, test_sigma]);
                let optimized_time = start.elapsed();
                println!("   Result: {optimized_result:.6}");
                println!("   Time: {optimized_time:.2?}");

                // Compare results and performance
                println!("\nğŸ“ˆ Performance Comparison:");
                println!(
                    "   Results match: {}",
                    (original_result - optimized_result).abs() < 1e-10
                );

                if optimized_time < original_time {
                    let speedup =
                        original_time.as_nanos() as f64 / optimized_time.as_nanos() as f64;
                    println!("   ğŸš€ Speedup: {speedup:.2}x faster");
                } else {
                    let slowdown =
                        optimized_time.as_nanos() as f64 / original_time.as_nanos() as f64;
                    println!("   ğŸŒ Slowdown: {slowdown:.2}x slower");
                }

                // The key insight: Î£(a*x + b*x) â†’ (a+b)*Î£(x) when a,b are independent of x
                // For log-density: Î£(-Â½ln(2Ï€) - ln(Ïƒ) - Â½((xáµ¢-Î¼)/Ïƒ)Â²)
                // â†’ n*(-Â½ln(2Ï€) - ln(Ïƒ)) + Î£(-Â½((xáµ¢-Î¼)/Ïƒ)Â²)
                println!("   ğŸ’¡ Sum splitting extracts constants from summation");
                println!("   ğŸ’¡ Constant terms computed once instead of per data point");
        }
    }

    #[cfg(not(feature = "optimization"))]
    {
        println!("\n3ï¸âƒ£ Sum Splitting Optimization");
        println!("-----------------------------");
        println!("âš ï¸  Optimization disabled - run with --features optimization");
    }

    // ============================================================================
    // 5. Composability Example
    // ============================================================================

    println!("\n5ï¸âƒ£ Composability Example");
    println!("------------------------");

    // Create a prior function: log p(Î¼) = -Â½Î¼Â² (standard normal prior on mean)
    let mut prior_ctx = DynamicContext::new();
    let mu_prior = prior_ctx.var();
    let log_prior = -0.5 * &mu_prior * &mu_prior;

    // Log posterior âˆ log prior + log likelihood
    // For demonstration, we'll show how these compose
    let test_prior = prior_ctx.eval(&log_prior, hlist![test_mu]);
    let test_posterior = test_prior + likelihood_result; // Simplified - normally would marginalize Ïƒ

    println!("âœ… Log prior p(Î¼=0): {test_prior:.6}");
    println!("âœ… Log likelihood: {likelihood_result:.6}");
    println!("âœ… Log posterior âˆ {test_posterior:.6}");

    println!("\nğŸ‰ Probabilistic demo completed successfully!");
    println!("   â€¢ Normal log-density function using LambdaVar");
    println!("   â€¢ IID sampling with symbolic summation");
    println!("   â€¢ Sum splitting optimization for efficient computation");
    println!("   â€¢ Composable probabilistic building blocks");

    Ok(())
}
