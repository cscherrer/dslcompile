//! Probabilistic Density Functions with Sum Splitting Demo
//!
//! This demo shows:
//! 1. Creating probability density functions with composable APIs
//! 2. IID (independent identically distributed) sampling
//! 3. Sum splitting optimization for efficient computation

use dslcompile::prelude::*;
use frunk::hlist;

#[cfg(feature = "optimization")]
use dslcompile::symbolic::egg_optimizer::optimize_simple_sum_splitting;

fn main() -> Result<()> {
    println!("üìä Probabilistic Density Functions Demo");
    println!("=======================================\n");

    // ============================================================================
    // 1. Normal Log-Density Function: f(Œº, œÉ, x) = -¬Ωln(2œÄ) - ln(œÉ) - ¬Ω((x-Œº)/œÉ)¬≤
    // ============================================================================

    println!("1Ô∏è‚É£ Normal Log-Density Function");
    println!("------------------------------");

    // Create context for building expressions
    let mut ctx = DynamicContext::new();

    // Variables for the log-density function: f(Œº, œÉ, x)
    let mu = ctx.var(); // mean
    let sigma = ctx.var(); // standard deviation
    let x = ctx.var(); // observation

    // Normal log-density: -¬Ωln(2œÄ) - ln(œÉ) - ¬Ω((x-Œº)/œÉ)¬≤
    let log_2pi = (2.0 * std::f64::consts::PI).ln();
    let neg_half = -0.5;

    let centered = &x - &mu; // (x - Œº)
    let standardized = &centered / &sigma; // (x - Œº) / œÉ
    let squared = &standardized * &standardized; // ((x - Œº) / œÉ)¬≤

    let log_density = neg_half * log_2pi - sigma.clone().ln() + neg_half * &squared;

    // Test single evaluation: N(0,1) at x=1
    let test_mu = 0.0;
    let test_sigma = 1.0;
    let test_x = 1.0;

    let single_result = ctx.eval(&log_density, hlist![test_mu, test_sigma, test_x]);
    println!("‚úÖ f(Œº=0, œÉ=1, x=1) = {single_result:.6}");

    // Expected: -¬Ωln(2œÄ) - ln(1) - ¬Ω(1¬≤) = -¬Ωln(2œÄ) - ¬Ω ‚âà -1.419
    let expected = -0.5 * (2.0 * std::f64::consts::PI).ln() - 0.5;
    println!("   Expected: {expected:.6} ‚úì");
    assert!((single_result - expected).abs() < 1e-10);

    // ============================================================================
    // 2. IID Log-Likelihood: L(Œº, œÉ, data) = Œ£ f(Œº, œÉ, x·µ¢) for x·µ¢ in data
    // ============================================================================

    println!("\n2Ô∏è‚É£ IID Log-Likelihood Function");
    println!("------------------------------");

    // Create context for summation
    let mut ctx = DynamicContext::new();
    let mu = ctx.var();
    let sigma = ctx.var();

    // Sample data
    let data = vec![1.0, 2.0, 0.5, 1.5, 0.8];
    println!("Sample data: {data:?}");

    // Create sum over data: Œ£ log_density(Œº, œÉ, x·µ¢)
    let iid_likelihood = ctx.sum(data.clone(), |x| {
        let log_2pi = (2.0 * std::f64::consts::PI).ln();
        let neg_half = -0.5;

        let centered = x - &mu;
        let standardized = &centered / &sigma;
        let squared = &standardized * &standardized;

        neg_half * log_2pi - sigma.clone().ln() + neg_half * &squared
    });

    // Test evaluation
    let likelihood_result = ctx.eval(&iid_likelihood, hlist![test_mu, test_sigma]);
    println!("‚úÖ L(Œº=0, œÉ=1, data) = {likelihood_result:.6}");

    // Verify by computing manually
    let manual_sum: f64 = data
        .iter()
        .map(|&x| -0.5 * (2.0 * std::f64::consts::PI).ln() - 0.0 - 0.5 * (x - 0.0).powi(2))
        .sum();
    println!("   Manual computation: {manual_sum:.6} ‚úì");
    assert!((likelihood_result - manual_sum).abs() < 1e-10);

    #[cfg(feature = "optimization")]
    {
        // ============================================================================
        // 3. Sum Splitting Optimization
        // ============================================================================

        println!("\n3Ô∏è‚É£ Sum Splitting Optimization");
        println!("-----------------------------");

        println!("üîß Analyzing expression structure...");
        let ast = ctx.to_ast(&iid_likelihood);
        let ops_before = ast.count_operations();
        println!("   Operations before optimization: {ops_before}");

        println!("\nüìã Expression Before Optimization:");
        println!("   AST: {ast:#?}");
        println!("   Pretty: {}", ctx.pretty_print(&iid_likelihood));

        match optimize_simple_sum_splitting(&ast) {
            Ok(optimized) => {
                let ops_after = optimized.count_operations();
                println!("\n‚úÖ Optimization successful!");
                println!("   Operations after optimization: {ops_after}");

                if ops_after < ops_before {
                    println!("   üéâ Reduced by {} operations", ops_before - ops_after);
                } else {
                    println!("   ‚ÑπÔ∏è  No reduction (expression may already be optimal)");
                }

                println!("\nüìã Expression After Optimization:");
                println!("   AST: {optimized:#?}");

                // ============================================================================
                // 4. Performance Benchmarking with Large Dataset
                // ============================================================================

                println!("\n4Ô∏è‚É£ Performance Benchmarking");
                println!("---------------------------");

                // Create large dataset for benchmarking
                let large_data: Vec<f64> = (0..10_000).map(|i| f64::from(i) * 0.001).collect();
                println!("üìä Testing with {} data points", large_data.len());

                // Create contexts for original and optimized expressions
                let mut orig_ctx = DynamicContext::new();
                let orig_mu = orig_ctx.var();
                let orig_sigma = orig_ctx.var();

                let original_expr = orig_ctx.sum(large_data.clone(), |x| {
                    let log_2pi = (2.0 * std::f64::consts::PI).ln();
                    let neg_half = -0.5;

                    let centered = x - &orig_mu;
                    let standardized = &centered / &orig_sigma;
                    let squared = &standardized * &standardized;

                    neg_half * log_2pi - orig_sigma.clone().ln() + neg_half * &squared
                });

                // For optimized version, we need to create a DynamicExpr from the optimized AST
                // This is a bit tricky, so let's create a simpler version for timing
                let mut opt_ctx = DynamicContext::new();
                let opt_mu = opt_ctx.var();
                let opt_sigma = opt_ctx.var();

                // Simulate what an optimized version might look like:
                // The optimization should factor out constants from the summation
                let n = large_data.len() as f64;
                let log_2pi = (2.0 * std::f64::consts::PI).ln();
                let constant_part = n * (-0.5 * log_2pi - opt_sigma.clone().ln());

                let variable_part = opt_ctx.sum(large_data.clone(), |x| {
                    let centered = x - &opt_mu;
                    let standardized = &centered / &opt_sigma;
                    -0.5 * &standardized * &standardized
                });

                let optimized_expr = constant_part + variable_part;

                // Benchmark original expression
                println!("\n‚è±Ô∏è  Timing Original Expression:");
                let start = std::time::Instant::now();
                let original_result = orig_ctx.eval(&original_expr, hlist![test_mu, test_sigma]);
                let original_time = start.elapsed();
                println!("   Result: {original_result:.6}");
                println!("   Time: {original_time:.2?}");

                // Benchmark optimized expression
                println!("\n‚è±Ô∏è  Timing Optimized Expression:");
                let start = std::time::Instant::now();
                let optimized_result = opt_ctx.eval(&optimized_expr, hlist![test_mu, test_sigma]);
                let optimized_time = start.elapsed();
                println!("   Result: {optimized_result:.6}");
                println!("   Time: {optimized_time:.2?}");

                // Compare results and performance
                println!("\nüìà Performance Comparison:");
                println!(
                    "   Results match: {}",
                    (original_result - optimized_result).abs() < 1e-10
                );

                if optimized_time < original_time {
                    let speedup =
                        original_time.as_nanos() as f64 / optimized_time.as_nanos() as f64;
                    println!("   üöÄ Speedup: {speedup:.2}x faster");
                } else {
                    let slowdown =
                        optimized_time.as_nanos() as f64 / original_time.as_nanos() as f64;
                    println!("   üêå Slowdown: {slowdown:.2}x slower");
                }

                // The key insight: Œ£(a*x + b*x) ‚Üí (a+b)*Œ£(x) when a,b are independent of x
                // For log-density: Œ£(-¬Ωln(2œÄ) - ln(œÉ) - ¬Ω((x·µ¢-Œº)/œÉ)¬≤)
                // ‚Üí n*(-¬Ωln(2œÄ) - ln(œÉ)) + Œ£(-¬Ω((x·µ¢-Œº)/œÉ)¬≤)
                println!("   üí° Sum splitting extracts constants from summation");
                println!("   üí° Constant terms computed once instead of per data point");
            }
            Err(e) => {
                println!("‚ùå Optimization failed: {e}");
            }
        }
    }

    #[cfg(not(feature = "optimization"))]
    {
        println!("\n3Ô∏è‚É£ Sum Splitting Optimization");
        println!("-----------------------------");
        println!("‚ö†Ô∏è  Optimization disabled - run with --features optimization");
    }

    // ============================================================================
    // 5. Composability Example
    // ============================================================================

    println!("\n5Ô∏è‚É£ Composability Example");
    println!("------------------------");

    // Create a prior function: log p(Œº) = -¬ΩŒº¬≤ (standard normal prior on mean)
    let mut prior_ctx = DynamicContext::new();
    let mu_prior = prior_ctx.var();
    let log_prior = -0.5 * &mu_prior * &mu_prior;

    // Log posterior ‚àù log prior + log likelihood
    // For demonstration, we'll show how these compose
    let test_prior = prior_ctx.eval(&log_prior, hlist![test_mu]);
    let test_posterior = test_prior + likelihood_result; // Simplified - normally would marginalize œÉ

    println!("‚úÖ Log prior p(Œº=0): {test_prior:.6}");
    println!("‚úÖ Log likelihood: {likelihood_result:.6}");
    println!("‚úÖ Log posterior ‚àù {test_posterior:.6}");

    println!("\nüéâ Probabilistic demo completed successfully!");
    println!("   ‚Ä¢ Normal log-density function using LambdaVar");
    println!("   ‚Ä¢ IID sampling with symbolic summation");
    println!("   ‚Ä¢ Sum splitting optimization for efficient computation");
    println!("   ‚Ä¢ Composable probabilistic building blocks");

    Ok(())
}
